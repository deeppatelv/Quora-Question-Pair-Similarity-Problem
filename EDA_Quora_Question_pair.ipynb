{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EDA- Quora Question pair.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-cfyuWP-0Ve",
        "colab_type": "text"
      },
      "source": [
        "# Quora question Pair Similarity Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPv7x5JO_BZE",
        "colab_type": "text"
      },
      "source": [
        "# 1. Exploratory Data Analysis on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4yub4RA-zBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from subprocess import check_output\n",
        "%matplotlib inline\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import distance\n",
        "from nltk.stem import PorterStemmer\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIM8ejp6_Iye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading data \n",
        "df = pd.read_csv(\"train.csv\")\n",
        "print(\"Number of data points:\",df.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_blFaama_UCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jVbOwCs_XhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEh6gVrb_h-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution of data points among output classes\n",
        "df.groupby(\"is_duplicate\")['id'].count().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwjhN5jd_6JG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Total number of question pairs for training:\\n   {}'.format(len(df)))\n",
        "print('Question pairs are not Similar (is_duplicate = 0):\\n   {}%'.format(100 - round(df['is_duplicate'].mean()*100, 2)))\n",
        "print('Question pairs are Similar (is_duplicate = 1):\\n   {}%'.format(round(df['is_duplicate'].mean()*100, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUQjH5yM_6Db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of Unique questions\n",
        "qids = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\n",
        "unique_qs = len(np.unique(qids))\n",
        "qs_morethan_onetime = np.sum(qids.value_counts() > 1)\n",
        "print ('Total number of  Unique Questions are: {}\\n'.format(unique_qs))\n",
        "print ('Number of unique questions that appear more than one time: {} ({}%)\\n'.format(qs_morethan_onetime,qs_morethan_onetime/unique_qs*100))\n",
        "print ('Max number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) \n",
        "q_vals=qids.value_counts()\n",
        "q_vals=q_vals.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLq5yqVbANgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting number of unique and repeated questions \n",
        "x = [\"unique_questions\" , \"Repeated Questions\"]\n",
        "y =  [unique_qs , qs_morethan_onetime]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title (\"Plot representing unique and repeated questions  \")\n",
        "sns.barplot(x,y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rFqawG__5-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checking whether there are any repeated pair of questions\n",
        "pair_duplicates = df[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()\n",
        "print (\"Number of duplicate questions\",(pair_duplicates).shape[0] - df.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqfyDlyK_57T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# umber of occurrences of each question\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.hist(qids.value_counts(), bins=160)\n",
        "plt.yscale('log', nonposy='clip')\n",
        "plt.title('Log-Histogram of question appearance counts')\n",
        "plt.xlabel('Number of occurences of question')\n",
        "plt.ylabel('Number of questions')\n",
        "print ('Maximum number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ygKf6QZAnV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checking whether there are any rows with null values\n",
        "nan_rows = df[df.isnull().any(1)]\n",
        "print (nan_rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PNmJOqyAqCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filling the null values with ' '\n",
        "df = df.fillna('')\n",
        "nan_rows = df[df.isnull().any(1)]\n",
        "print (nan_rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOGaUuOGBAUR",
        "colab_type": "text"
      },
      "source": [
        "* CONSTRUCTING SOME NEW FEATURES:-\n",
        "\n",
        " - ____freq_qid1____ = Frequency of qid1's\n",
        " - ____freq_qid2____ = Frequency of qid2's \n",
        " - ____q1len____ = Length of q1\n",
        " - ____q2len____ = Length of q2\n",
        " - ____q1_n_words____ = Number of words in Question 1\n",
        " - ____q2_n_words____ = Number of words in Question 2\n",
        " - ____word_Common____ = (Number of common unique words in Question 1 and Question 2)\n",
        " - ____word_Total____ =(Total num of words in Question 1 + Total num of words in Question 2)\n",
        " - ____word_share____ = (word_common)/(word_Total)\n",
        " - ____freq_q1+freq_q2____ = sum total of frequency of qid1 and qid2 \n",
        " - ____freq_q1-freq_q2____ = absolute difference of frequency of qid1 and qid2  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXJhGRucAsGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count') \n",
        "df['freq_qid2'] = df.groupby('qid2')['qid2'].transform('count')\n",
        "df['q1len'] = df['question1'].str.len() \n",
        "df['q2len'] = df['question2'].str.len()\n",
        "df['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n",
        "df['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n",
        "\n",
        "def normalized_word_Common(row):\n",
        "    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
        "    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
        "    return 1.0 * len(w1 & w2)\n",
        "df['word_Common'] = df.apply(normalized_word_Common, axis=1)\n",
        "\n",
        "def normalized_word_Total(row):\n",
        "    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
        "    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
        "    return 1.0 * (len(w1) + len(w2))\n",
        "df['word_Total'] = df.apply(normalized_word_Total, axis=1)\n",
        "\n",
        "def normalized_word_share(row):\n",
        "    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
        "    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
        "    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
        "df['word_share'] = df.apply(normalized_word_share, axis=1)\n",
        "\n",
        "df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2']\n",
        "df['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2'])\n",
        "\n",
        "df.to_csv(\"df_fe_without_preprocessing_train.csv\", index=False)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZg4RklrBv0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analysis of some of word_share\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(1,2,1)\n",
        "sns.violinplot(x = 'is_duplicate', y = 'word_share', data = df[0:])\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(df[df['is_duplicate'] == 1.0]['word_share'][0:] , label = \"1\", color = 'red')\n",
        "sns.distplot(df[df['is_duplicate'] == 0.0]['word_share'][0:] , label = \"0\" , color = 'blue' )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIWV49aMBvrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analysis of some of word_common\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(1,2,1)\n",
        "sns.violinplot(x = 'is_duplicate', y = 'word_Common', data = df[0:])\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(df[df['is_duplicate'] == 1.0]['word_Common'][0:] , label = \"1\", color = 'red')\n",
        "sns.distplot(df[df['is_duplicate'] == 0.0]['word_Common'][0:] , label = \"0\" , color = 'blue' )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ14X0uoB_L_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from subprocess import check_output\n",
        "%matplotlib inline\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import distance\n",
        "from nltk.stem import PorterStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import distance\n",
        "from nltk.stem import PorterStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "from fuzzywuzzy import fuzz\n",
        "from sklearn.manifold import TSNE\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from os import path\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbFxqzQMCtIl",
        "colab_type": "text"
      },
      "source": [
        "- Preprocessing:\n",
        "    - Removing html tags \n",
        "    - Removing Punctuations\n",
        "    - Performing stemming\n",
        "    - Removing Stopwords\n",
        "    - Expanding contractions etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW5hhF6EB-7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To get the results in 4 decemal points\n",
        "SAFE_DIV = 0.0001 \n",
        "\n",
        "STOP_WORDS = stopwords.words(\"english\")\n",
        "\n",
        "# performing above mentioned preprocessing steps\n",
        "def preprocess(x):\n",
        "    x = str(x).lower()\n",
        "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
        "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
        "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
        "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
        "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
        "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
        "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
        "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
        "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
        "    \n",
        "    \n",
        "    porter = PorterStemmer()\n",
        "    pattern = re.compile('\\W')\n",
        "    \n",
        "    if type(x) == type(''):\n",
        "        x = re.sub(pattern, ' ', x)\n",
        "    \n",
        "    \n",
        "    if type(x) == type(''):\n",
        "        x = porter.stem(x)\n",
        "        example1 = BeautifulSoup(x)\n",
        "        x = example1.get_text()\n",
        "               \n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yev-nl4yC_lj",
        "colab_type": "text"
      },
      "source": [
        "# Advanced Feature Extraction (NLP and Fuzzy Features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaHW3Z_mDIHx",
        "colab_type": "text"
      },
      "source": [
        "- __Token__: You get a token by splitting sentence a space\n",
        "- __Stop_Word__ : stop words as per NLTK.\n",
        "- __Word__ : A token that is not a stop_word\n",
        "\n",
        "\n",
        "Features:\n",
        "- __cwc_min__ :  Ratio of common_word_count to min lenghth of word count of Q1 and Q2 <br>cwc_min = common_word_count / (min(len(q1_words), len(q2_words))\n",
        "<br>\n",
        "- __cwc_max__ :  Ratio of common_word_count to max lenghth of word count of Q1 and Q2 <br>cwc_max = common_word_count / (max(len(q1_words), len(q2_words))\n",
        "<br>\n",
        "- __csc_min__ :  Ratio of common_stop_count to min lenghth of stop count of Q1 and Q2 <br> csc_min = common_stop_count / (min(len(q1_stops), len(q2_stops))\n",
        "<br>\n",
        "- __csc_max__ :  Ratio of common_stop_count to max lenghth of stop count of Q1 and Q2<br>csc_max = common_stop_count / (max(len(q1_stops), len(q2_stops))\n",
        "<br>\n",
        "- __ctc_min__ :  Ratio of common_token_count to min lenghth of token count of Q1 and Q2<br>ctc_min = common_token_count / (min(len(q1_tokens), len(q2_tokens))\n",
        "<br>\n",
        "- __ctc_max__ :  Ratio of common_token_count to max lenghth of token count of Q1 and Q2<br>ctc_max = common_token_count / (max(len(q1_tokens), len(q2_tokens))\n",
        "<br>  \n",
        "- __last_word_eq__ :  Check if First word of both questions is equal or not<br>last_word_eq = int(q1_tokens[-1] == q2_tokens[-1])\n",
        "<br>\n",
        "- __first_word_eq__ :  Check if First word of both questions is equal or not<br>first_word_eq = int(q1_tokens[0] == q2_tokens[0])\n",
        "<br>  \n",
        "- __abs_len_diff__ :  Abs. length difference<br>abs_len_diff = abs(len(q1_tokens) - len(q2_tokens))\n",
        "<br>\n",
        "- __mean_len__ :  Average Token Length of both Questions<br>mean_len = (len(q1_tokens) + len(q2_tokens))/2\n",
        "<br>\n",
        "- __longest_substr_ratio__ :  Ratio of length longest common substring to min lenghth of token count of Q1 and Q2<br>longest_substr_ratio = len(longest common substring) / (min(len(q1_tokens), len(q2_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsolwZyUDDvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_token_features(q1, q2):\n",
        "    token_features = [0.0]*10\n",
        "    \n",
        "    # Converting the Sentence into Tokens: \n",
        "    q1_tokens = q1.split()\n",
        "    q2_tokens = q2.split()\n",
        "\n",
        "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
        "        return token_features\n",
        "    # Get the non-stopwords in Questions\n",
        "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
        "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
        "    \n",
        "    #Get the stopwords in Questions\n",
        "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
        "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
        "    \n",
        "    # Get the common non-stopwords from Question pair\n",
        "    common_word_count = len(q1_words.intersection(q2_words))\n",
        "    \n",
        "    # Get the common stopwords from Question pair\n",
        "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
        "    \n",
        "    # Get the common Tokens from Question pair\n",
        "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
        "    \n",
        "    \n",
        "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
        "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
        "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
        "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
        "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
        "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
        "    \n",
        "    # Last word of both question is same or not\n",
        "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
        "    \n",
        "    # First word of both question is same or not\n",
        "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
        "    \n",
        "    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n",
        "    \n",
        "    #Average Token Length of both Questions\n",
        "    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n",
        "    return token_features\n",
        "\n",
        "# get the Longest Common sub string\n",
        "\n",
        "def get_longest_substr_ratio(a, b):\n",
        "    strs = list(distance.lcsubstrings(a, b))\n",
        "    if len(strs) == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n",
        "\n",
        "def extract_features(df):\n",
        "    # preprocessing each question\n",
        "    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n",
        "    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n",
        "\n",
        "    print(\"token features...\")\n",
        "    \n",
        "    # Merging Features with dataset\n",
        "    \n",
        "    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    \n",
        "    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
        "    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
        "    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
        "    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
        "    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
        "    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
        "    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
        "    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
        "    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n",
        "    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n",
        "   \n",
        "    #Computing Fuzzy Features and Merging with Dataset\n",
        "    print(\"fuzzy features..\")\n",
        "\n",
        "    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n",
        "    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n",
        "    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHl-JihjC-7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Extracting features for train:\")\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "df = extract_features(df)\n",
        "df.to_csv(\"nlp_features_train.csv\", index=False)\n",
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPLB_FkIECym",
        "colab_type": "text"
      },
      "source": [
        "* Pair plot of features ['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajNF_o_jC4b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = df.shape[0]\n",
        "sns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3zPnFtTEMTj",
        "colab_type": "text"
      },
      "source": [
        "* Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp_KcmGQEHXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using TSNE for Dimentionality reduction for 15 Features(Generated after cleaning the data) to 3 dimention\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "dfp_subsampled = df[0:5000]\n",
        "X = MinMaxScaler().fit_transform(dfp_subsampled[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\n",
        "y = dfp_subsampled['is_duplicate'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCo3kH2hEOeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tsne2d = TSNE(n_components=2,init='random',random_state=101,method='barnes_hut',n_iter=1000,verbose=2,angle=0.5).fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rezxIpdUEX14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n",
        "# draw the plot in appropriate place in the grid\n",
        "sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,palette=\"Set1\",markers=['s','o'])\n",
        "plt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}